{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.4"},"colab":{"name":"train.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"Q_Hwsxmymlxr","executionInfo":{"status":"ok","timestamp":1628344965801,"user_tz":180,"elapsed":15291,"user":{"displayName":"Evelyn Batista","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiE2KTV235hvCLjlsBmJm7_F_NZ4HJObPSJ2nXV=s64","userId":"01315954064004223909"}},"outputId":"9e94db0a-2ec6-4132-89fa-30c0c7aad016","colab":{"base_uri":"https://localhost:8080/"}},"source":["!pip install scipy==1.1.0"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting scipy==1.1.0\n","  Downloading scipy-1.1.0-cp37-cp37m-manylinux1_x86_64.whl (31.2 MB)\n","\u001b[K     |████████████████████████████████| 31.2 MB 51 kB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from scipy==1.1.0) (1.19.5)\n","Installing collected packages: scipy\n","  Attempting uninstall: scipy\n","    Found existing installation: scipy 1.4.1\n","    Uninstalling scipy-1.4.1:\n","      Successfully uninstalled scipy-1.4.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","pymc3 3.11.2 requires scipy>=1.2.0, but you have scipy 1.1.0 which is incompatible.\n","plotnine 0.6.0 requires scipy>=1.2.0, but you have scipy 1.1.0 which is incompatible.\n","albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Successfully installed scipy-1.1.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1SnYottAmnE-","executionInfo":{"status":"ok","timestamp":1628344971066,"user_tz":180,"elapsed":5279,"user":{"displayName":"Evelyn Batista","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiE2KTV235hvCLjlsBmJm7_F_NZ4HJObPSJ2nXV=s64","userId":"01315954064004223909"}},"outputId":"3e1c5955-d7fa-4e71-aee3-5172fe49d69b","colab":{"base_uri":"https://localhost:8080/"}},"source":["!pip install pygame"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting pygame\n","  Downloading pygame-2.0.1-cp37-cp37m-manylinux1_x86_64.whl (11.8 MB)\n","\u001b[K     |████████████████████████████████| 11.8 MB 135 kB/s \n","\u001b[?25hInstalling collected packages: pygame\n","Successfully installed pygame-2.0.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"F61BCajxBp6y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605630453875,"user_tz":180,"elapsed":32640,"user":{"displayName":"Evelyn Batista","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjY8IGxgHK3bDmbakYOmbG9B_UCYufI2e20n8BhHQ=s64","userId":"06299154530663245421"}},"outputId":"9457c541-c26c-477b-9fb3-2a29f9f8c5c5"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","workdir_path = '/content/drive/My Drive/exercicios_dqn/DQN'\n","os.chdir(workdir_path)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eagWX1xIA_o0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605630482452,"user_tz":180,"elapsed":5535,"user":{"displayName":"Evelyn Batista","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjY8IGxgHK3bDmbakYOmbG9B_UCYufI2e20n8BhHQ=s64","userId":"06299154530663245421"}},"outputId":"6ac9f967-b314-4683-d47e-97fe9d3397c8"},"source":["#!pip install scipy==1.1.0\n","#!pip install pygame\n","from __future__ import division, print_function\n","import keras\n","from keras.models import Sequential\n","from keras.models import load_model\n","from keras.layers.core import Activation, Dense, Flatten, Dropout\n","from keras.layers.convolutional import Conv2D\n","from keras.optimizers import Adam\n","from scipy.misc import imresize\n","import collections\n","import numpy as np\n","import os\n","\n","import wrapped_game as wp\n","print(keras.__version__)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting pygame\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/4c/2ebe8ab1a695a446574bc48d96eb3503649893be8c769e7fafd65fd18833/pygame-2.0.0-cp36-cp36m-manylinux1_x86_64.whl (11.5MB)\n","\u001b[K     |████████████████████████████████| 11.5MB 5.4MB/s \n","\u001b[?25hInstalling collected packages: pygame\n","Successfully installed pygame-2.0.0\n","pygame 2.0.0 (SDL 2.0.12, python 3.6.9)\n","Hello from the pygame community. https://www.pygame.org/contribute.html\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Rh50u1LgA_pA"},"source":["# DQN - Deep Q-Learning"]},{"cell_type":"markdown","metadata":{"id":"mQyvQ5CuA_pB"},"source":["<img src=\"DQN.png\" />"]},{"cell_type":"markdown","metadata":{"id":"mRggT-UmA_pB"},"source":["## Pré processamento"]},{"cell_type":"markdown","metadata":{"id":"FMmnq6YAA_pC"},"source":["A entrada vem em um conjunto de quatro 800 x 800 imagens, então a forma da entrada é (4, 800, 800). No entanto, a rede espera sua entrada como um tensor de forma quadrangular (tamanho do lote, 80, 80, 4). No começo do jogo, não temos quatro quadros, portanto, falsificamos o empilhamento do primeiro quadro quatro vezes. A forma do tensor de saída retornado dessa função é (80, 80, 4).\n","\n","\n","A única diferença é o tamanho da entrada e da saída. Nossa forma de entrada é (80, 80, 4) enquanto a deles (Deep Mind) era (84, 84, 4) e nossa saída é (3) correspondente às três ações para as quais o valor da função Q precisa ser computado, enquanto que elas foram (18), correspondente às ações possíveis da Atari."]},{"cell_type":"code","metadata":{"id":"Z8T7tCEjA_pD"},"source":["def preprocess_images(images):\n","    if images.shape[0] < 4:\n","        # 1 imagem\n","        x_t = images[0]\n","        x_t = imresize(x_t, (80, 80))\n","        x_t = x_t.astype(\"float\")\n","        x_t /= 255.0\n","        s_t = np.stack((x_t, x_t, x_t, x_t), axis=2)\n","    else:\n","        # 4 imagens\n","        xt_list = []\n","        for i in range(images.shape[0]):\n","            x_t = imresize(images[i], (80, 80))\n","            x_t = x_t.astype(\"float\")\n","            x_t /= 255.0\n","            xt_list.append(x_t)\n","        s_t = np.stack((xt_list[0], xt_list[1], xt_list[2], xt_list[3]), \n","                       axis=2)\n","    s_t = np.expand_dims(s_t, axis=0)\n","    return s_t"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-g91VMZGA_pG"},"source":["def get_next_batch(experience, model, num_actions, gamma, batch_size):\n","    \n","    #amostra aleatória\n","    batch_indices = np.random.randint(low=0, high=len(experience), size=batch_size)\n","        \n","    #inicializa X e Y e o batch de acordo com os indices\n","    \n","    batch = [experience[i] for i in batch_indices]\n","    \n","    X = np.zeros((batch_size, 80, 80, 4))\n","    Y = np.zeros((batch_size, num_actions))\n","    \n","    for i in range(len(batch)):\n","        s_t, a_t, r_t, s_tp1, game_over = batch[i]\n","        X[i] = s_t\n","        Y[i] = model.predict(s_t)[0]\n","        Q_sa = np.max(model.predict(s_tp1)[0])\n","        if game_over:\n","            Y[i, a_t] = r_t\n","        else:\n","            Y[i, a_t] = r_t + gamma * Q_sa\n","    return X, Y"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-9_pFU6FA_pK"},"source":["### Inicializa os parâmetros"]},{"cell_type":"markdown","metadata":{"id":"AJjPGTW_A_pL"},"source":["Cada época corresponde a um único jogo ou episódio"]},{"cell_type":"code","metadata":{"id":"NN2n3MlcA_pM"},"source":["\n","DATA_DIR = \"../data\"\n","NUM_ACTIONS = 3 # número de ações de saída que a rede pode enviar para o jogo (esquerda, faz nada, direita) (0, 1 e 2)\n","\n","GAMMA =  0.99 # fator de desconto para recompensas futuras\n","#referem-se a valores iniciais e finais para o parâmetro em exploração de base\n","INITIAL_EPSILON = 1.0\n","FINAL_EPSILON = 0.001\n","MEMORY_SIZE =  1000 # tamanho da mamória de repetição de experiência\n","NUM_EPOCHS_OBSERVE = 200 #refere-se ao número de épocas onde a rede tem permissão para explorar o jogo , \n","                            #enviando-lhe ações completamente aleatórias e vendo as recompensas\n","NUM_EPOCHS_TRAIN = 2000 #refere-se ao número de épocas em que a rede será submetida a treinamento on-line\n","\n","\n","BATCH_SIZE = 64 #tamanho do mini-lote que usaremos para treinamento\n","NUM_EPOCHS = NUM_EPOCHS_OBSERVE + NUM_EPOCHS_TRAIN #número total de jogos jogados para o treinamento"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pyxe_JRkA_pP"},"source":["### constrói um modelo"]},{"cell_type":"markdown","metadata":{"id":"q5xm7oLJA_pP"},"source":["Existem três camadas convolucionais e duas camadas totalmente conectadas (Dense). Todas as camadas, exceto a última, possuem a unidade de ativação ReLU. Como estamos prevendo valores de funções Q, temos uma rede de regressão e a última camada não possui unidade de ativação.\n","\n","A função de perda é a diferença quadrática entre o valor atual de Q (s, a) e seu valor calculado em termos da soma da recompensa e do valor Q descontado Q (s ', a') um passo no futuro, então a função de perda é a de erro quadrado médio (MSE).\n","\n","Adam -> um bom otimizador de propósito geral, instanciado com uma baixa taxa de aprendizado."]},{"cell_type":"code","metadata":{"id":"PLyaALAeA_pQ"},"source":["model = Sequential()\n","model.add(Conv2D(32, kernel_size=8, strides=4,\n","                 kernel_initializer=\"normal\",\n","                 padding=\"same\",\n","                 input_shape=(80, 80, 4)))\n","model.add(Activation(\"relu\"))\n","model.add(Conv2D(64, kernel_size=4, strides=2, \n","                 kernel_initializer=\"normal\",\n","                 padding=\"same\"))\n","model.add(Activation(\"relu\"))\n","model.add(Conv2D(64, kernel_size=3, strides=1,\n","                 kernel_initializer=\"normal\",\n","                 padding=\"same\"))\n","model.add(Activation(\"relu\"))\n","model.add(Flatten())\n","model.add(Dense(512, kernel_initializer=\"normal\"))\n","model.add(Activation(\"relu\"))\n","\n","model.add(Dense(3, kernel_initializer=\"normal\"))\n","\n","model.compile(optimizer=Adam(lr=1e-6), loss=\"mse\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l1wnyKusA_pU"},"source":["###### game, file para salvar, inicializa epsilon "]},{"cell_type":"code","metadata":{"id":"PDxEgzT4A_pU"},"source":["game = wp.MyWrappedGame()\n","experience = collections.deque(maxlen=MEMORY_SIZE)\n","\n","fout = open(\"rl-network-results_2.tsv\", \"w\")\n","num_games, num_wins = 0, 0\n","epsilon = INITIAL_EPSILON"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L4jyr-x7A_pY"},"source":["## Treino"]},{"cell_type":"markdown","metadata":{"id":"Wdh4CPvIA_pZ"},"source":["Um jogo corresponde a um único episódio de uma bola que cai do teto e está sendo pego pela pá ou está sendo perdido. A perda é a diferença quadrática entre o valor Q previsto e real para o jogo.\n","\n","Se estivermos no modo de observação, vamos apenas gerar um número aleatório correspondente a um de nossas ações, caso contrário, usaremos a exploração para selecionar uma ação aleatória ou usar nossa rede neural (que também estamos treinando) para prever a ação que devemos enviar.\n","\n","Quando a rede é relativamente não treinada, suas previsões não são muito boas, então faz sentido explorar o espaço de estados mais em um esforço para reduzir as chances de ficar preso em um mínimo local. No entanto, à medida que a rede é cada vez mais treinada, reduzimos o valor de forma gradual para que o modelo consiga prever mais e mais ações que a rede envia para o jogo.\n","\n"]},{"cell_type":"code","metadata":{"id":"o7mAkWYoA_pa","colab":{"base_uri":"https://localhost:8080/","height":758},"executionInfo":{"status":"error","timestamp":1605630589841,"user_tz":180,"elapsed":18928,"user":{"displayName":"Evelyn Batista","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjY8IGxgHK3bDmbakYOmbG9B_UCYufI2e20n8BhHQ=s64","userId":"06299154530663245421"}},"outputId":"bc0d2206-af7d-4f9b-8dac-3302eb913a2d"},"source":["for e in range(NUM_EPOCHS):\n","    #restabelecemos o estado do jogo neste momento (cada época é um jogo)\n","    loss = 0.0\n","    game.reset()\n","\n","    # pega o primeiro estado\n","    a_0 = 1  # (0 = esquerda, 1 = faz nada, 2 = direita)\n","    x_t, r_0, game_over = game.step(a_0) \n","    s_t = preprocess_images(x_t)\n","\n","    while not game_over:\n","        s_tm1 = s_t #guarda estado atual\n","        #  próxima ação\n","        if e <= NUM_EPOCHS_OBSERVE:\n","            a_t = np.random.randint(low=0, high=NUM_ACTIONS, size=1)[0]\n","        else:\n","            if np.random.rand() <= epsilon:\n","                a_t = np.random.randint(low=0, high=NUM_ACTIONS, size=1)[0]\n","            else:\n","                q = model.predict(s_t)[0]\n","                a_t = np.argmax(q)\n","\n","        # faz a ação, pega a recompensa\n","        x_t, r_t, game_over = game.step(a_t)\n","        s_t = preprocess_images(x_t)\n","        # se for recompensado, incrementa numero de vitorias\n","        if r_t == 1:\n","            num_wins += 1          \n","                        \n","        # guarda a experiência\n","        experience.append((s_tm1, a_t, r_t, s_t, game_over))  #(estado, ação, recompensa, novo estado, fim) \n","        \n","        ### Se memoria cheia, remove o primeiro elemento\n","        if len(experience) > MEMORY_SIZE:\n","                experience = experience[1:]\n","\n","        if e > NUM_EPOCHS_OBSERVE:\n","            # acaba abservação, começa o treinamento\n","            # calcula gradiente descendente\n","            X, Y = get_next_batch(experience, model, NUM_ACTIONS, \n","                                      GAMMA, BATCH_SIZE)\n","            \n","            loss += model.train_on_batch(X, Y)\n","\n","    \n","    # reduz o epsilon\n","    if epsilon > FINAL_EPSILON:\n","        epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / NUM_EPOCHS\n","        \n","    \n","    #Prints e Saves\n","    print(\"Epoca {:04d}/{:d} | Perda {:.5f} | Vezes ganhas: {:d} | epsilon {:.5f}\"\n","            .format(e + 1, NUM_EPOCHS, loss, num_wins, epsilon))\n","    fout.write(\"{:04d}\\t{:.5f}\\t{:d}\\n\"\n","            .format(e + 1, loss, num_wins))\n","               \n","    # só para salvar estado atual do modelo em um arquivo               \n","    if e % 100 == 0:\n","        model.save(\"rl-network_2.h5\", overwrite=True)\n","        \n","        \n","fout.close()\n","#salva modelo final pra nao precisar executar novamente\n","model.save(\"rl-network_2.h5\", overwrite=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: `imresize` is deprecated!\n","`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n","Use ``skimage.transform.resize`` instead.\n","  \"\"\"\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: `imresize` is deprecated!\n","`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n","Use ``skimage.transform.resize`` instead.\n","  del sys.path[0]\n"],"name":"stderr"},{"output_type":"stream","text":["Epoca 0001/700 | Perda 0.00000 | Vezes ganhas: 0 | epsilon 0.09986\n","Epoca 0002/700 | Perda 0.00000 | Vezes ganhas: 0 | epsilon 0.09972\n","Epoca 0003/700 | Perda 0.00000 | Vezes ganhas: 1 | epsilon 0.09958\n","Epoca 0004/700 | Perda 0.00000 | Vezes ganhas: 1 | epsilon 0.09943\n","Epoca 0005/700 | Perda 0.00000 | Vezes ganhas: 1 | epsilon 0.09929\n","Epoca 0006/700 | Perda 0.00000 | Vezes ganhas: 1 | epsilon 0.09915\n","Epoca 0007/700 | Perda 0.00000 | Vezes ganhas: 1 | epsilon 0.09901\n","Epoca 0008/700 | Perda 0.00000 | Vezes ganhas: 1 | epsilon 0.09887\n","Epoca 0009/700 | Perda 0.00000 | Vezes ganhas: 2 | epsilon 0.09873\n","Epoca 0010/700 | Perda 0.00000 | Vezes ganhas: 2 | epsilon 0.09859\n","Epoca 0011/700 | Perda 0.00000 | Vezes ganhas: 2 | epsilon 0.09844\n","Epoca 0012/700 | Perda 0.00000 | Vezes ganhas: 3 | epsilon 0.09830\n","Epoca 0013/700 | Perda 0.00000 | Vezes ganhas: 3 | epsilon 0.09816\n","Epoca 0014/700 | Perda 0.00000 | Vezes ganhas: 3 | epsilon 0.09802\n","Epoca 0015/700 | Perda 0.00000 | Vezes ganhas: 3 | epsilon 0.09788\n","Epoca 0016/700 | Perda 0.00000 | Vezes ganhas: 3 | epsilon 0.09774\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-7cf7990c5d9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# faz a ação, pega a recompensa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mx_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame_over\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0ms_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# se for recompensado, incrementa numero de vitorias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/My Drive/A_Doutorado/codigos/others/DRL_aula_bi/DQN/wrapped_game.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_over\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_over\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"DkmCM9UVCPo6"},"source":[""],"execution_count":null,"outputs":[]}]}