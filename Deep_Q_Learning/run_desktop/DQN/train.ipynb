{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers.core import Activation, Dense, Flatten, Dropout\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.optimizers import Adam\n",
    "from scipy.misc import imresize\n",
    "import collections\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import wrapped_game as wp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN - Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"DQN.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A entrada vem em um conjunto de quatro 800 x 800 imagens, então a forma da entrada é (4, 800, 800). No entanto, a rede espera sua entrada como um tensor de forma quadrangular (tamanho do lote, 80, 80, 4). No começo do jogo, não temos quatro quadros, portanto, falsificamos o empilhamento do primeiro quadro quatro vezes. A forma do tensor de saída retornado dessa função é (80, 80, 4).\n",
    "\n",
    "\n",
    "A única diferença é o tamanho da entrada e da saída. Nossa forma de entrada é (80, 80, 4) enquanto a deles (Deep Mind) era (84, 84, 4) e nossa saída é (3) correspondente às três ações para as quais o valor da função Q precisa ser computado, enquanto que elas foram (18), correspondente às ações possíveis da Atari."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(images):\n",
    "    if images.shape[0] < 4:\n",
    "        # 1 imagem\n",
    "        x_t = images[0]\n",
    "        x_t = imresize(x_t, (80, 80))\n",
    "        x_t = x_t.astype(\"float\")\n",
    "        x_t /= 255.0\n",
    "        s_t = np.stack((x_t, x_t, x_t, x_t), axis=2)\n",
    "    else:\n",
    "        # 4 imagens\n",
    "        xt_list = []\n",
    "        for i in range(images.shape[0]):\n",
    "            x_t = imresize(images[i], (80, 80))\n",
    "            x_t = x_t.astype(\"float\")\n",
    "            x_t /= 255.0\n",
    "            xt_list.append(x_t)\n",
    "        s_t = np.stack((xt_list[0], xt_list[1], xt_list[2], xt_list[3]), \n",
    "                       axis=2)\n",
    "    s_t = np.expand_dims(s_t, axis=0)\n",
    "    return s_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_batch(experience, model, num_actions, gamma, batch_size):\n",
    "    \n",
    "    #amostra aleatória\n",
    "    batch_indices = np.random.randint(low=0, high=len(experience), size=batch_size)\n",
    "        \n",
    "    #inicializa X e Y e o batch de acordo com os indices\n",
    "    \n",
    "    batch = [experience[i] for i in batch_indices]\n",
    "    \n",
    "    X = np.zeros((batch_size, 80, 80, 4))\n",
    "    Y = np.zeros((batch_size, num_actions))\n",
    "    \n",
    "    for i in range(len(batch)):\n",
    "        s_t, a_t, r_t, s_tp1, game_over = batch[i]\n",
    "        X[i] = s_t\n",
    "        Y[i] = model.predict(s_t)[0]\n",
    "        Q_sa = np.max(model.predict(s_tp1)[0])\n",
    "        if game_over:\n",
    "            Y[i, a_t] = r_t\n",
    "        else:\n",
    "            Y[i, a_t] = r_t + gamma * Q_sa\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inicializa os parâmetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada época corresponde a um único jogo ou episódio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_DIR = \"../data\"\n",
    "NUM_ACTIONS = 3 # número de ações de saída que a rede pode enviar para o jogo (esquerda, faz nada, direita) (0, 1 e 2)\n",
    "\n",
    "GAMMA =  # fator de desconto para recompensas futuras\n",
    "#referem-se a valores iniciais e finais para o parâmetro em exploração de base\n",
    "INITIAL_EPSILON = \n",
    "FINAL_EPSILON = \n",
    "MEMORY_SIZE =  # tamanho da mamória de repetição de experiência\n",
    "NUM_EPOCHS_OBSERVE =  #refere-se ao número de épocas onde a rede tem permissão para explorar o jogo , \n",
    "                            #enviando-lhe ações completamente aleatórias e vendo as recompensas\n",
    "NUM_EPOCHS_TRAIN =  #refere-se ao número de épocas em que a rede será submetida a treinamento on-line\n",
    "\n",
    "\n",
    "BATCH_SIZE =  #tamanho do mini-lote que usaremos para treinamento\n",
    "NUM_EPOCHS = NUM_EPOCHS_OBSERVE + NUM_EPOCHS_TRAIN #número total de jogos jogados para o treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### constrói um modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existem três camadas convolucionais e duas camadas totalmente conectadas (Dense). Todas as camadas, exceto a última, possuem a unidade de ativação ReLU. Como estamos prevendo valores de funções Q, temos uma rede de regressão e a última camada não possui unidade de ativação.\n",
    "\n",
    "A função de perda é a diferença quadrática entre o valor atual de Q (s, a) e seu valor calculado em termos da soma da recompensa e do valor Q descontado Q (s ', a') um passo no futuro, então a função de perda é a de erro quadrado médio (MSE).\n",
    "\n",
    "Adam -> um bom otimizador de propósito geral, instanciado com uma baixa taxa de aprendizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=8, strides=4,\n",
    "                 kernel_initializer=\"normal\",\n",
    "                 padding=\"same\",\n",
    "                 input_shape=(80, 80, 4)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Conv2D(64, kernel_size=4, strides=2, \n",
    "                 kernel_initializer=\"normal\",\n",
    "                 padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Conv2D(64, kernel_size=3, strides=1,\n",
    "                 kernel_initializer=\"normal\",\n",
    "                 padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, kernel_initializer=\"normal\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "model.add(Dense(3, kernel_initializer=\"normal\"))\n",
    "\n",
    "model.compile(optimizer=Adam(lr=1e-6), loss=\"mse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### game, file para salvar, inicializa epsilon "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = wp.MyWrappedGame()\n",
    "experience = collections.deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "fout = open(\"rl-network-results_2.tsv\", \"w\")\n",
    "num_games, num_wins = 0, 0\n",
    "epsilon = INITIAL_EPSILON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treino"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um jogo corresponde a um único episódio de uma bola que cai do teto e está sendo pego pela pá ou está sendo perdido. A perda é a diferença quadrática entre o valor Q previsto e real para o jogo.\n",
    "\n",
    "Se estivermos no modo de observação, vamos apenas gerar um número aleatório correspondente a um de nossas ações, caso contrário, usaremos a exploração para selecionar uma ação aleatória ou usar nossa rede neural (que também estamos treinando) para prever a ação que devemos enviar.\n",
    "\n",
    "Quando a rede é relativamente não treinada, suas previsões não são muito boas, então faz sentido explorar o espaço de estados mais em um esforço para reduzir as chances de ficar preso em um mínimo local. No entanto, à medida que a rede é cada vez mais treinada, reduzimos o valor de forma gradual para que o modelo consiga prever mais e mais ações que a rede envia para o jogo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(NUM_EPOCHS):\n",
    "    #restabelecemos o estado do jogo neste momento (cada época é um jogo)\n",
    "    loss = 0.0\n",
    "    game.reset()\n",
    "\n",
    "    # pega o primeiro estado\n",
    "    a_0 = 1  # (0 = esquerda, 1 = faz nada, 2 = direita)\n",
    "    x_t, r_0, game_over = game.step(a_0) \n",
    "    s_t = preprocess_images(x_t)\n",
    "\n",
    "    while not game_over:\n",
    "        s_tm1 = s_t #guarda estado atual\n",
    "        #  próxima ação\n",
    "        if e <= NUM_EPOCHS_OBSERVE:\n",
    "            a_t = np.random.randint(low=0, high=NUM_ACTIONS, size=1)[0]\n",
    "        else:\n",
    "            if np.random.rand() <= epsilon:\n",
    "                a_t = np.random.randint(low=0, high=NUM_ACTIONS, size=1)[0]\n",
    "            else:\n",
    "                q = model.predict(s_t)[0]\n",
    "                a_t = np.argmax(q)\n",
    "\n",
    "        # faz a ação, pega a recompensa\n",
    "        x_t, r_t, game_over = game.step(a_t)\n",
    "        s_t = preprocess_images(x_t)\n",
    "        # se for recompensado, incrementa numero de vitorias\n",
    "        if r_t == 1:\n",
    "            num_wins += 1          \n",
    "                        \n",
    "        # guarda a experiência\n",
    "        experience.append((s_tm1, a_t, r_t, s_t, game_over))  #(estado, ação, recompensa, novo estado, fim) \n",
    "        \n",
    "        ### Se memoria cheia, remove o primeiro elemento\n",
    "        if len(experience) > MEMORY_SIZE:\n",
    "                experience = experience[1:]\n",
    "\n",
    "        if e > NUM_EPOCHS_OBSERVE:\n",
    "            # acaba abservação, começa o treinamento\n",
    "            # calcula gradiente descendente\n",
    "            X, Y = get_next_batch(experience, model, NUM_ACTIONS, \n",
    "                                      GAMMA, BATCH_SIZE)\n",
    "            \n",
    "            loss += model.train_on_batch(X, Y)\n",
    "\n",
    "    \n",
    "    # reduz o epsilon\n",
    "    if epsilon > FINAL_EPSILON:\n",
    "        epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / NUM_EPOCHS\n",
    "        \n",
    "    \n",
    "    #Prints e Saves\n",
    "    print(\"Epoca {:04d}/{:d} | Perda {:.5f} | Vezes ganhas: {:d} | epsilon {:.5f}\"\n",
    "            .format(e + 1, NUM_EPOCHS, loss, num_wins, epsilon))\n",
    "    fout.write(\"{:04d}\\t{:.5f}\\t{:d}\\n\"\n",
    "            .format(e + 1, loss, num_wins))\n",
    "               \n",
    "    # só para salvar estado atual do modelo em um arquivo               \n",
    "    if e % 100 == 0:\n",
    "        model.save(\"rl-network_2.h5\", overwrite=True)\n",
    "        \n",
    "        \n",
    "fout.close()\n",
    "#salva modelo final pra nao precisar executar novamente\n",
    "model.save(\"rl-network_2.h5\", overwrite=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
